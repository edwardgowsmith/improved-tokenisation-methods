{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb46867",
   "metadata": {},
   "source": [
    "This notebook contains the code for reproducing the intrinsic evaluation in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c062c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requirements\n",
    "\n",
    "import sentencepiece as spm\n",
    "import os, urllib.request, zipfile, re, string\n",
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e13241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sentencepiece models \n",
    "\n",
    "bpe_default = spm.SentencePieceProcessor(os.path.join('..', 'models', 'sentencepiece', 'bpe_default', 'wikipedia_spm3_bpe_16000.model'))\n",
    "bpe_prime = spm.SentencePieceProcessor(os.path.join('..', 'models', 'sentencepiece', 'bpe_prime', 'wikipedia_spm1_bpe_16000.model'))\n",
    "unigram_default = spm.SentencePieceProcessor(os.path.join('..', 'models', 'sentencepiece', 'unigram_default', 'wikipedia_spm3_unigram_16000.model'))\n",
    "unigram_prime = spm.SentencePieceProcessor(os.path.join('..', 'models', 'sentencepiece', 'unigram_prime', 'wikipedia_spm1_unigram_16000.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b17ede06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the morphological datasets\n",
    "#LADEC\n",
    "urllib.request.urlretrieve(\"https://era.library.ualberta.ca/items/dc3b9033-14d0-48d7-b6fa-6398a30e61e4/download/830937da-a00b-4735-8cf2-3c67d5cc6d50\", \"LADECv1-2019.csv\")\n",
    "\n",
    "#MorphoLex\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/hugomailhot/MorphoLex-en/master/MorphoLEX_en.xlsx\", \"MorphoLEX_en.xlsx\")\n",
    "\n",
    "#MorphyNet\n",
    "urllib.request.urlretrieve(\"https://github.com/kbatsuren/MorphyNet/blob/main/eng/eng.derivational.v1.tsv?raw=true\", \"eng.derivational.v1.tsv\")\n",
    "\n",
    "#DagoBERT\n",
    "urllib.request.urlretrieve(\"http://cistern.cis.lmu.de/dagobert/dagobert_data.zip\", \"dagobert_data.zip\")\n",
    "with zipfile.ZipFile(\"dagobert_data.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"dagobert_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca984c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e48c871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get_boundaries from a tokenisation\n",
    "def get_boundaries(y):\n",
    "    boundaries = [len(''.join(y[:i])) for i in range(1, len(y))]\n",
    "    return boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3bcb148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 6, 9]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example\n",
    "get_boundaries([\"un\", \"help\", \"ful\", \"ness\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3e7d715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenisers = {'bpe_default': bpe_default, 'bpe_prime': bpe_prime, 'unigram_default': unigram_default, 'unigram_prime': unigram_prime}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cae4eb",
   "metadata": {},
   "source": [
    "### Quantitative Evaluation of Morphological Correctness (Tables 1 and 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c4970c",
   "metadata": {},
   "source": [
    "#### LADEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1590cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read LADEC dataset\n",
    "ladec_df = pd.read_csv(\"LADECv1-2019.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08203fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate entries (we only want those with a unique morphological parse)\n",
    "ladec_df = ladec_df[~ladec_df.duplicated('stim', keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15c96668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7804"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of entries in the dataset with a unique morphological parse \n",
    "len(ladec_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b44c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n",
      "2.981547924141466 bpe_default\n",
      "2.5991799077396207 bpe_prime\n",
      "2.80228088159918 unigram_default\n",
      "2.6652998462327013 unigram_prime\n",
      "\n",
      "precision\n",
      "41.211846870150026 bpe_default\n",
      "53.79807692307692 bpe_prime\n",
      "51.93032349804479 unigram_default\n",
      "56.717451523545705 unigram_prime\n",
      "\n",
      "recall\n",
      "81.66324961558175 bpe_default\n",
      "86.03280369041518 bpe_prime\n",
      "93.59302921578677 unigram_default\n",
      "94.45156330087134 unigram_prime\n",
      "\n",
      "f1\n",
      "54.77909575382499 bpe_default\n",
      "66.19996056004733 bpe_prime\n",
      "66.79775024006585 unigram_default\n",
      "70.875 unigram_prime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics for each of the tokenisers on the LADEC dataset\n",
    "# This generates the results in the LADEC column of Table 1 in the paper\n",
    "\n",
    "x = ladec_df[['c1', 'c2', 'stim']]\n",
    "\n",
    "tps = defaultdict(int)\n",
    "fps = defaultdict(int)\n",
    "fns = defaultdict(int)\n",
    "tns = defaultdict(int)\n",
    "lengths = defaultdict(int)\n",
    "\n",
    "for _, row in x.iterrows():\n",
    "    # Iterate over the tokenisers \n",
    "    for key, val in tokenisers.items():\n",
    "        # Gold standard morphological segmentation from the dataset\n",
    "        gstandard = [row['c1'], row['c2']]\n",
    "        spm = val\n",
    "        \n",
    "        # Tokenise the compound with the given tokeniser, stripping space symbols\n",
    "        y = [x.lstrip('▁') for x in spm.encode(row['stim'], out_type=str) if x != \"▁\" ]\n",
    "        \n",
    "        # Get the boundaries for the gold standard and the tokeniser\n",
    "        gstandard_boundaries = get_boundaries(gstandard)\n",
    "        y_boundaries = get_boundaries(y)\n",
    "        fn = 0\n",
    "        for i in y_boundaries:\n",
    "            if i in gstandard_boundaries:\n",
    "                # True positives are those appearing in both generated and reference\n",
    "                tps[key] += 1\n",
    "            else:\n",
    "                # False positives are those appearing in the generated but not the reference\n",
    "                fps[key] += 1\n",
    "        for i in gstandard_boundaries:\n",
    "            if i not in y_boundaries:\n",
    "                # False negatives are those appearing in the reference but not the generated\n",
    "                fn += 1\n",
    "                \n",
    "        tns[key] += (len(row['stim']) - 1 - len(y_boundaries) - fn)\n",
    "        fns[key] += fn\n",
    "                \n",
    "        lengths[key] += len(y)\n",
    "                \n",
    "\n",
    "\n",
    "print('length')\n",
    "for k in tokenisers.keys():\n",
    "    print(lengths[k] / len(x), k)\n",
    "print()\n",
    "\n",
    "print('precision')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fps[k]), k)\n",
    "print()\n",
    "\n",
    "print('recall')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fns[k]), k)\n",
    "print()\n",
    "\n",
    "print('f1')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * (tps[k] / (tps[k] + 0.5*(fps[k] + fns[k]))), k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc61496",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2054ce68",
   "metadata": {},
   "source": [
    "#### MorphoLex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8d661b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\edward\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:315: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "\n",
    "dfs = []\n",
    "for i in range(1, 33):\n",
    "    dfs += [pd.read_excel('MorphoLex_en.xlsx', sheet_name=i)]\n",
    "\n",
    "morpho_db = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cee96a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep revelant columns\n",
    "x = morpho_db[['Nmorph', 'Word', 'MorphoLexSegm', 'PRS_signature']]\n",
    "# Remove NaN entries\n",
    "x = x[x['Nmorph'] > 1]\n",
    "# Keep only the entries with a concatenative parse \n",
    "x = x[x['MorphoLexSegm'].apply(lambda x: ''.join(re.split('\\<|\\>|\\}|\\{|\\(|\\)', x))) == x['Word']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d056d1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n",
      "2.668856002660459 bpe_default\n",
      "2.467741935483871 bpe_prime\n",
      "2.5575324243431994 unigram_default\n",
      "2.651064183571666 unigram_prime\n",
      "\n",
      "precision\n",
      "43.391620584865244 bpe_default\n",
      "50.77602809561572 bpe_prime\n",
      "58.081562933703424 unigram_default\n",
      "53.859710962284105 unigram_prime\n",
      "\n",
      "recall\n",
      "57.59820129612485 bpe_default\n",
      "59.277873297182914 bpe_prime\n",
      "71.95476788784552 unigram_default\n",
      "70.73138473746859 unigram_prime\n",
      "\n",
      "f1\n",
      "49.49566699815315 bpe_default\n",
      "54.69855992189407 bpe_prime\n",
      "64.27811909262759 unigram_default\n",
      "61.1531974500443 unigram_prime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform evaluation on the whole of the dataset first\n",
    "# This generates the results in the MorphoLex column of Table 1 in the paper\n",
    "\n",
    "tps = defaultdict(int)\n",
    "fps = defaultdict(int)\n",
    "fns = defaultdict(int)\n",
    "tns = defaultdict(int)\n",
    "lengths = defaultdict(int)\n",
    "\n",
    "def get_boundaries(y):\n",
    "    boundaries = [len(''.join(y[:i])) for i in range(1, len(y))]\n",
    "    return boundaries\n",
    "    \n",
    "for _, row in x.iterrows():\n",
    "    seg = row['MorphoLexSegm']\n",
    "    gstandard = [y for y in re.split('\\<|\\>|\\}|\\{|\\(|\\)', seg) if y != '']\n",
    "    gstandard_boundaries = get_boundaries(gstandard)\n",
    "\n",
    "    for key, val in tokenisers.items():\n",
    "        \n",
    "        spm = val\n",
    "        \n",
    "        y = [x.lstrip('▁') for x in spm.encode(row['Word'], out_type=str) if x != \"▁\" ]\n",
    "        y_boundaries = get_boundaries(y)\n",
    "        fn = 0\n",
    "        for i in y_boundaries:\n",
    "            if i in gstandard_boundaries:\n",
    "                tps[key] += 1\n",
    "            else:\n",
    "                fps[key] += 1\n",
    "        for i in gstandard_boundaries:\n",
    "            if i not in y_boundaries:\n",
    "                fn += 1\n",
    "                \n",
    "        tns[key] += (len(row['Word']) - 1 - len(y_boundaries) - fn)\n",
    "        fns[key] += fn\n",
    "                \n",
    "        lengths[key] += len(y)\n",
    "                \n",
    "print('length')\n",
    "for k in tokenisers.keys():\n",
    "    print(lengths[k] / len(x), k)\n",
    "print()\n",
    "\n",
    "print('precision')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fps[k]), k)\n",
    "print()\n",
    "\n",
    "print('recall')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fns[k]), k)\n",
    "print()\n",
    "\n",
    "print('f1')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * (tps[k] / (tps[k] + 0.5*(fps[k] + fns[k]))), k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "100ec8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation on the prefix only entries\n",
    "\n",
    "morpholex_only_prefix = x[x['PRS_signature'].isin(['1,1,0', '1,2,0', '1,3,0', '2,1,0'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b7da6643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n",
      "2.5437382001258655 bpe_default\n",
      "2.2580239144115795 bpe_prime\n",
      "2.509754562617999 unigram_default\n",
      "2.478288231592196 unigram_prime\n",
      "\n",
      "precision\n",
      "33.50998777007746 bpe_default\n",
      "50.3751875937969 bpe_prime\n",
      "53.438932888703626 unigram_default\n",
      "57.17326521924223 unigram_prime\n",
      "\n",
      "recall\n",
      "50.27522935779817 bpe_default\n",
      "61.59021406727829 bpe_prime\n",
      "78.40978593272172 unigram_default\n",
      "82.14067278287462 unigram_prime\n",
      "\n",
      "f1\n",
      "40.21526418786693 bpe_default\n",
      "55.4210236653825 bpe_prime\n",
      "63.559742191373324 unigram_default\n",
      "67.41967871485943 unigram_prime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This generates results in \"Only Prefixes\" column of Table 2 in the paper\n",
    "\n",
    "tps = defaultdict(int)\n",
    "fps = defaultdict(int)\n",
    "fns = defaultdict(int)\n",
    "tns = defaultdict(int)\n",
    "lengths = defaultdict(int)\n",
    "\n",
    "def get_boundaries(y):\n",
    "    boundaries = [len(''.join(y[:i])) for i in range(1, len(y))]\n",
    "    return boundaries\n",
    "    \n",
    "for _, row in morpholex_only_prefix.iterrows():\n",
    "    seg = row['MorphoLexSegm']\n",
    "    gstandard = [y for y in re.split('\\<|\\>|\\}|\\{|\\(|\\)', seg) if y != '']\n",
    "    gstandard_boundaries = get_boundaries(gstandard)\n",
    "\n",
    "    for key, val in tokenisers.items():\n",
    "        \n",
    "        spm = val\n",
    "        \n",
    "        y = [x.lstrip('▁') for x in spm.encode(row['Word'], out_type=str) if x != \"▁\" ]\n",
    "        y_boundaries = get_boundaries(y)\n",
    "        fn = 0\n",
    "        for i in y_boundaries:\n",
    "            if i in gstandard_boundaries:\n",
    "                tps[key] += 1\n",
    "            else:\n",
    "                fps[key] += 1\n",
    "        for i in gstandard_boundaries:\n",
    "            if i not in y_boundaries:\n",
    "                fn += 1\n",
    "                \n",
    "        tns[key] += (len(row['Word']) - 1 - len(y_boundaries) - fn)\n",
    "        fns[key] += fn\n",
    "                \n",
    "        lengths[key] += len(y)\n",
    "                \n",
    "print('length')\n",
    "for k in tokenisers.keys():\n",
    "    print(lengths[k] / len(morpholex_only_prefix), k)\n",
    "print()\n",
    "\n",
    "print('precision')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fps[k]), k)\n",
    "print()\n",
    "\n",
    "print('recall')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fns[k]), k)\n",
    "print()\n",
    "\n",
    "print('f1')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * (tps[k] / (tps[k] + 0.5*(fps[k] + fns[k]))), k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66b9ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform evaluation on the suffix only entries\n",
    "morpholex_only_suffix = x[x['PRS_signature'].isin(['0,1,1', '0,1,2', '0,1,3',  '0,2,1', '0,2,2', '0,2,3', '0,3,1'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e04838e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n",
      "2.3336697909156454 bpe_default\n",
      "2.1720980533525593 bpe_prime\n",
      "2.217808219178082 unigram_default\n",
      "2.3860129776496035 unigram_prime\n",
      "\n",
      "precision\n",
      "11.990485457887338 bpe_default\n",
      "14.449160361690348 bpe_prime\n",
      "15.860517435320585 unigram_default\n",
      "14.117769454848107 unigram_prime\n",
      "\n",
      "recall\n",
      "74.00734067400734 bpe_default\n",
      "78.37837837837837 bpe_prime\n",
      "89.3893893893894 unigram_default\n",
      "90.55722389055722 unigram_prime\n",
      "\n",
      "f1\n",
      "20.637357525005815 bpe_default\n",
      "24.400124649423496 bpe_prime\n",
      "26.940868865647627 unigram_default\n",
      "24.427343503892715 unigram_prime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This generates results in \"Only Suffixes\" column of Table 2 in the paper\n",
    "\n",
    "tps = defaultdict(int)\n",
    "fps = defaultdict(int)\n",
    "fns = defaultdict(int)\n",
    "tns = defaultdict(int)\n",
    "lengths = defaultdict(int)\n",
    "\n",
    "def get_boundaries(y):\n",
    "    boundaries = [len(''.join(y[:i])) for i in range(1, len(y))]\n",
    "    return boundaries\n",
    "    \n",
    "for _, row in morpholex_only_suffix.iterrows():\n",
    "    seg = row['MorphoLexSegm']\n",
    "    gstandard = [y for y in re.split('\\<|\\>|\\}|\\{|\\(|\\)', seg) if y != '']\n",
    "    gstandard_boundaries = get_boundaries(gstandard)\n",
    "\n",
    "    for key, val in tokenisers.items():\n",
    "        \n",
    "        spm = val\n",
    "        \n",
    "        y = [x.lstrip('▁') for x in spm.encode(row['Word'], out_type=str) if x != \"▁\" ]\n",
    "        y_boundaries = get_boundaries(y)\n",
    "        fn = 0\n",
    "        for i in y_boundaries:\n",
    "            if i in gstandard_boundaries:\n",
    "                tps[key] += 1\n",
    "            else:\n",
    "                fps[key] += 1\n",
    "        for i in gstandard_boundaries:\n",
    "            if i not in y_boundaries:\n",
    "                fn += 1\n",
    "                \n",
    "        tns[key] += (len(row['Word']) - 1 - len(y_boundaries) - fn)\n",
    "        fns[key] += fn\n",
    "                \n",
    "        lengths[key] += len(y)\n",
    "                \n",
    "print('length')\n",
    "for k in tokenisers.keys():\n",
    "    print(lengths[k] / len(morpholex_only_suffix), k)\n",
    "print()\n",
    "\n",
    "print('precision')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fps[k]), k)\n",
    "print()\n",
    "\n",
    "print('recall')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fns[k]), k)\n",
    "print()\n",
    "\n",
    "print('f1')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * (tps[k] / (tps[k] + 0.5*(fps[k] + fns[k]))), k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7cde8224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2692"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of entries containing prefixes\n",
    "len(x[x['PRS_signature'].isin(['1,1,0', '1,1,1', '1,1,2', '1,1,3','1,2,0', '1,2,1', '1,2,2', '1,2,3', '1,3,0', '2,1,0', '2,1,1',\n",
    "       '2,1,2'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0129eb41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7422"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of entries containing suffixes\n",
    "len(x[x['PRS_signature'].isin(['0,1,1', '0,1,2', '0,1,3','0,2,1', '0,2,2', '0,2,3', '0,3,1', '1,1,1', '1,1,2', '1,1,3', '1,2,1', '1,2,2', '1,2,3', '2,1,1', '2,1,2'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2a0a03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e93e32f",
   "metadata": {},
   "source": [
    "####  MorphyNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "496a0e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "morphynet_der = pd.read_csv('eng.derivational.v1.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ec22b1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\AppData\\Local\\Temp/ipykernel_7312/1901335959.py:21: FutureWarning: Possible nested set at position 2\n",
      "  gstandard = list(filter(None, re.split(f\"({d})\", row[1])))\n",
      "C:\\Users\\Edward\\AppData\\Local\\Temp/ipykernel_7312/1901335959.py:21: FutureWarning: Possible nested set at position 16\n",
      "  gstandard = list(filter(None, re.split(f\"({d})\", row[1])))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n",
      "3.170682846875819 bpe_default\n",
      "2.9342738227965053 bpe_prime\n",
      "3.0859766091742142 unigram_default\n",
      "3.032545495733595 unigram_prime\n",
      "\n",
      "precision\n",
      "19.89155064496397 bpe_default\n",
      "24.620755125350637 bpe_prime\n",
      "32.298464069775804 unigram_default\n",
      "33.59828141783029 unigram_prime\n",
      "\n",
      "recall\n",
      "53.27525540037196 bpe_default\n",
      "59.19054954226277 bpe_prime\n",
      "83.33135847754073 unigram_default\n",
      "84.60282215479091 unigram_prime\n",
      "\n",
      "f1\n",
      "28.96743750884127 bpe_default\n",
      "34.77612075828064 bpe_prime\n",
      "46.55329963117579 unigram_default\n",
      "48.09615716917418 unigram_prime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This generates the results in the MorphyNet column of Table 1 in the paper\n",
    "\n",
    "x = morphynet_der[[0, 1]]\n",
    "\n",
    "tps = defaultdict(int)\n",
    "fps = defaultdict(int)\n",
    "fns = defaultdict(int)\n",
    "tns = defaultdict(int)\n",
    "lengths = defaultdict(int)\n",
    "\n",
    "\n",
    "def get_boundaries(y):\n",
    "    boundaries = [len(''.join(y[:i])) for i in range(1, len(y))]\n",
    "    return boundaries\n",
    "\n",
    "length = 0 \n",
    "\n",
    "for _, row in x.iterrows():\n",
    "    d = row[0]\n",
    "    try:\n",
    "        gstandard = list(filter(None, re.split(f\"({d})\", row[1])))\n",
    "    except:\n",
    "        continue\n",
    "    # We don't want entries consisting of a single morpheme\n",
    "    if len(gstandard) == 1 or ''.join(gstandard) != row[1]:\n",
    "        continue\n",
    "    gstandard_boundaries = get_boundaries(gstandard)   \n",
    "    \n",
    "    for key, val in tokenisers.items():\n",
    "        \n",
    "        spm = val\n",
    "        y = [x.lstrip('▁') for x in spm.encode(row[1], out_type=str) if x != \"▁\" ]\n",
    "        y_boundaries = get_boundaries(y)\n",
    "        fn = 0\n",
    "        for i in y_boundaries:\n",
    "            if i in gstandard_boundaries:\n",
    "                tps[key] += 1\n",
    "            else:\n",
    "                fps[key] += 1\n",
    "        for i in gstandard_boundaries:\n",
    "            if i not in y_boundaries:\n",
    "                fn += 1\n",
    "                \n",
    "        tns[key] += (len(row[1]) - 1 - len(y_boundaries) - fn)\n",
    "        fns[key] += fn\n",
    "                \n",
    "        lengths[key] += len(y)\n",
    "    \n",
    "    length += 1\n",
    "\n",
    "print('length')\n",
    "for k in tokenisers.keys():\n",
    "    print(lengths[k] / len(x), k)\n",
    "print()\n",
    "\n",
    "print('precision')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fps[k]), k)\n",
    "print()\n",
    "\n",
    "print('recall')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fns[k]), k)\n",
    "print()\n",
    "\n",
    "print('f1')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * (tps[k] / (tps[k] + 0.5*(fps[k] + fns[k]))), k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85549d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fa8e6c2",
   "metadata": {},
   "source": [
    "#### DagoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6ff45fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Edward\\AppData\\Local\\Temp/ipykernel_7312/2082753429.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  dfs += [pd.read_csv(os.path.join('dagobert_data', f'bin{i}.txt'), sep='\\|\\|\\|', header=None)]\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for i in range(1, 8):\n",
    "    dfs += [pd.read_csv(os.path.join('dagobert_data', f'bin{i}.txt'), sep='\\|\\|\\|', header=None)]\n",
    "\n",
    "dagobert_db = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "facb30a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length\n",
      "3.2155287482599313 bpe_default\n",
      "2.8623941197310363 bpe_prime\n",
      "3.160619518112817 unigram_default\n",
      "2.8098538879127406 unigram_prime\n",
      "\n",
      "precision\n",
      "28.396429455720554 bpe_default\n",
      "37.421687245818774 bpe_prime\n",
      "45.33219456645518 unigram_default\n",
      "54.52958401799176 unigram_prime\n",
      "\n",
      "recall\n",
      "60.08691797200355 bpe_default\n",
      "66.78208710604798 bpe_prime\n",
      "93.58694484000108 unigram_default\n",
      "94.62371369461835 unigram_prime\n",
      "\n",
      "f1\n",
      "38.56666767263493 bpe_default\n",
      "47.96560187669986 bpe_prime\n",
      "61.07871975731326 unigram_default\n",
      "69.18776621281309 unigram_prime\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This generates the results in the DagoBERT column of Table 1 in the paper\n",
    "\n",
    "x = dagobert_db[~dagobert_db.duplicated(1)][[1, 2]]\n",
    "\n",
    "tps = defaultdict(int)\n",
    "fps = defaultdict(int)\n",
    "fns = defaultdict(int)\n",
    "tns = defaultdict(int)\n",
    "lengths = defaultdict(int)\n",
    "\n",
    "\n",
    "for _, row in x.iterrows():\n",
    "    d = row[2]\n",
    "    try:\n",
    "        gstandard = list(filter(None, re.split(f\"({d})\", row[1])))\n",
    "    except:\n",
    "        continue\n",
    "    if len(gstandard) == 1:\n",
    "        continue\n",
    "    gstandard_boundaries = get_boundaries(gstandard)\n",
    "    \n",
    "        \n",
    "    for key, val in tokenisers.items():\n",
    "        \n",
    "        spm = val\n",
    "        y = [x.lstrip('▁') for x in spm.encode(row[1], out_type=str) if x != \"▁\" ]\n",
    "        y_boundaries = get_boundaries(y)\n",
    "        fn = 0\n",
    "        for i in y_boundaries:\n",
    "            if i in gstandard_boundaries:\n",
    "                tps[key] += 1\n",
    "            else:\n",
    "                fps[key] += 1\n",
    "        for i in gstandard_boundaries:\n",
    "            if i not in y_boundaries:\n",
    "                fn += 1\n",
    "                \n",
    "        tns[key] += (len(row[1]) - 1 - len(y_boundaries) - fn)\n",
    "        fns[key] += fn\n",
    "                \n",
    "        lengths[key] += len(y)\n",
    "    length += 1\n",
    "                \n",
    "\n",
    "print('length')\n",
    "for k in tokenisers.keys():\n",
    "    print(lengths[k] / len(x), k)\n",
    "print()\n",
    "\n",
    "print('precision')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fps[k]), k)\n",
    "print()\n",
    "\n",
    "print('recall')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * tps[k] / (tps[k] + fns[k]), k)\n",
    "print()\n",
    "\n",
    "print('f1')\n",
    "for k in tokenisers.keys():\n",
    "    print(100 * (tps[k] / (tps[k] + 0.5*(fps[k] + fns[k]))), k)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92742164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc5ad711",
   "metadata": {},
   "source": [
    "### Qualitative Evaluation of Morphological Correctness (Table 3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cfeaee15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>bpe_default</th>\n",
       "      <th>bpe_prime</th>\n",
       "      <th>unigram_default</th>\n",
       "      <th>unigram_prime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>directional</td>\n",
       "      <td>▁direction, al</td>\n",
       "      <td>direction, al</td>\n",
       "      <td>▁direction, al</td>\n",
       "      <td>direction, al</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>unidirectional</td>\n",
       "      <td>▁un, id, ire, ction, al</td>\n",
       "      <td>un, id, ire, ction, al</td>\n",
       "      <td>▁un, i, direct, ional</td>\n",
       "      <td>uni, direction, al</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>electroneutral</td>\n",
       "      <td>▁elect, r, one, ut, ral</td>\n",
       "      <td>electr, one, utr, al</td>\n",
       "      <td>▁electron, eu, tral</td>\n",
       "      <td>electro, neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neurotransmitter</td>\n",
       "      <td>▁neuro, trans, mit, ter</td>\n",
       "      <td>neuro, transmitter</td>\n",
       "      <td>▁neuro, trans, mitt, er</td>\n",
       "      <td>neuro, transmitter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>responsiveness</td>\n",
       "      <td>▁respons, iveness</td>\n",
       "      <td>respons, iveness</td>\n",
       "      <td>▁re, s, pon, s, ive, ness</td>\n",
       "      <td>r, e, sp, on, s, ive, ness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hyporesponsiveness</td>\n",
       "      <td>▁hyp, ores, p, ons, iveness</td>\n",
       "      <td>hypo, respons, iveness</td>\n",
       "      <td>▁hypo, res, pon, s, ive, ness</td>\n",
       "      <td>hypo, r, e, sp, on, s, ive, ness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hyperresponsiveness</td>\n",
       "      <td>▁hyper, resp, ons, iveness</td>\n",
       "      <td>hyper, respons, iveness</td>\n",
       "      <td>▁hyper, res, pon, s, ive, ness</td>\n",
       "      <td>hyper, r, e, sp, on, s, ive, ness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>saturated</td>\n",
       "      <td>▁sat, urated</td>\n",
       "      <td>sat, urated</td>\n",
       "      <td>▁sat, ur, ated</td>\n",
       "      <td>saturated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>unsaturated</td>\n",
       "      <td>▁uns, atur, ated</td>\n",
       "      <td>un, sat, urated</td>\n",
       "      <td>▁un, sa, tur, ated</td>\n",
       "      <td>un, saturated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>equal</td>\n",
       "      <td>▁equal</td>\n",
       "      <td>equal</td>\n",
       "      <td>▁equal</td>\n",
       "      <td>equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>unequal</td>\n",
       "      <td>▁un, equ, al</td>\n",
       "      <td>une, qual</td>\n",
       "      <td>▁un, e, qual</td>\n",
       "      <td>un, equal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>multiplayer</td>\n",
       "      <td>▁multip, layer</td>\n",
       "      <td>multi, player</td>\n",
       "      <td>▁multi, play, er</td>\n",
       "      <td>multi, player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>nonmultiplayer</td>\n",
       "      <td>▁non, m, ult, ip, layer</td>\n",
       "      <td>non, multi, player</td>\n",
       "      <td>▁non, mul, ti, play, er</td>\n",
       "      <td>non, multi, player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>overpriced</td>\n",
       "      <td>▁over, p, ric, ed</td>\n",
       "      <td>over, pr, iced</td>\n",
       "      <td>▁over, p, ric, ed</td>\n",
       "      <td>over, price, d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>accessible</td>\n",
       "      <td>▁accessible</td>\n",
       "      <td>accessible</td>\n",
       "      <td>▁accessible</td>\n",
       "      <td>accessible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>unaccessible</td>\n",
       "      <td>▁un, ac, cess, ible</td>\n",
       "      <td>un, accessible</td>\n",
       "      <td>▁un, ac, ces, s, ible</td>\n",
       "      <td>un, accessible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>unicycle</td>\n",
       "      <td>▁un, icy, cle</td>\n",
       "      <td>un, icy, cle</td>\n",
       "      <td>▁un, i, cycle</td>\n",
       "      <td>uni, cycle</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  input                  bpe_default                bpe_prime  \\\n",
       "0           directional               ▁direction, al            direction, al   \n",
       "1        unidirectional      ▁un, id, ire, ction, al   un, id, ire, ction, al   \n",
       "2        electroneutral      ▁elect, r, one, ut, ral     electr, one, utr, al   \n",
       "3      neurotransmitter      ▁neuro, trans, mit, ter       neuro, transmitter   \n",
       "4        responsiveness            ▁respons, iveness         respons, iveness   \n",
       "5    hyporesponsiveness  ▁hyp, ores, p, ons, iveness   hypo, respons, iveness   \n",
       "6   hyperresponsiveness   ▁hyper, resp, ons, iveness  hyper, respons, iveness   \n",
       "7             saturated                 ▁sat, urated              sat, urated   \n",
       "8           unsaturated             ▁uns, atur, ated          un, sat, urated   \n",
       "9                 equal                       ▁equal                    equal   \n",
       "10              unequal                 ▁un, equ, al                une, qual   \n",
       "11          multiplayer               ▁multip, layer            multi, player   \n",
       "12       nonmultiplayer      ▁non, m, ult, ip, layer       non, multi, player   \n",
       "13           overpriced            ▁over, p, ric, ed           over, pr, iced   \n",
       "14           accessible                  ▁accessible               accessible   \n",
       "15         unaccessible          ▁un, ac, cess, ible           un, accessible   \n",
       "16             unicycle                ▁un, icy, cle             un, icy, cle   \n",
       "\n",
       "                   unigram_default                      unigram_prime  \n",
       "0                   ▁direction, al                      direction, al  \n",
       "1            ▁un, i, direct, ional                 uni, direction, al  \n",
       "2              ▁electron, eu, tral                   electro, neutral  \n",
       "3          ▁neuro, trans, mitt, er                 neuro, transmitter  \n",
       "4        ▁re, s, pon, s, ive, ness         r, e, sp, on, s, ive, ness  \n",
       "5    ▁hypo, res, pon, s, ive, ness   hypo, r, e, sp, on, s, ive, ness  \n",
       "6   ▁hyper, res, pon, s, ive, ness  hyper, r, e, sp, on, s, ive, ness  \n",
       "7                   ▁sat, ur, ated                          saturated  \n",
       "8               ▁un, sa, tur, ated                      un, saturated  \n",
       "9                           ▁equal                              equal  \n",
       "10                    ▁un, e, qual                          un, equal  \n",
       "11                ▁multi, play, er                      multi, player  \n",
       "12         ▁non, mul, ti, play, er                 non, multi, player  \n",
       "13               ▁over, p, ric, ed                     over, price, d  \n",
       "14                     ▁accessible                         accessible  \n",
       "15           ▁un, ac, ces, s, ible                     un, accessible  \n",
       "16                   ▁un, i, cycle                         uni, cycle  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This generates the data in Table 3 of the paper\n",
    "\n",
    "inputs = [\"directional\", \"unidirectional\", \"electroneutral\", \"neurotransmitter\", \"responsiveness\", \"hyporesponsiveness\",\n",
    "         \"hyperresponsiveness\", \"saturated\", \"unsaturated\", \"equal\", \"unequal\", \"multiplayer\", \"nonmultiplayer\", \"overpriced\",\n",
    "         \"accessible\", \"unaccessible\", \"unicycle\"]\n",
    "table_3 = pd.DataFrame(columns=[\"input\", \"bpe_default\", \"bpe_prime\", \"unigram_default\", \"unigram_prime\"])\n",
    "\n",
    "for string in inputs:\n",
    "    row = {\n",
    "            'input': string, \n",
    "            'bpe_default': ', '.join(bpe_default.encode(string, out_type=str)),\n",
    "            'bpe_prime': ', '.join(bpe_prime.encode(string, out_type=str)),\n",
    "            'unigram_default': ', '.join(unigram_default.encode(string, out_type=str)),\n",
    "            'unigram_prime': ', '.join(unigram_prime.encode(string, out_type=str))\n",
    "          }\n",
    "    table_3 = table_3.append(row, ignore_index=True)\n",
    "\n",
    "table_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf8d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also define a function to generate tokenisations for each of the four tokenisers given inputs\n",
    "\n",
    "def generate_tokenisations(strs):\n",
    "    for x in strs:\n",
    "        print((', '.join(bpe_default.encode(x, out_type=str))))\n",
    "        print((', '.join(bpe_prime.encode(x, out_type=str))))\n",
    "        print((', '.join(unigram_default.encode(x, out_type=str))))\n",
    "        print((', '.join(unigram_prime.encode(x, out_type=str))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f8bdfd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁this, is, as, ent, enc, eth, at, ne, ed, st, ob, ese, g, mented\n",
      "this, is, as, ent, ence, that, need, sto, b, ese, g, mented\n",
      "▁this, isa, s, ent, ence, that, ne, ed, s, to, be, s, eg, ment, ed\n",
      "this, is, a, sentence, that, needs, to, be, segment, e, d\n"
     ]
    }
   ],
   "source": [
    "# Example segmentation from the paper\n",
    "generate_tokenisations([\"thisisasentencethatneedstobesegmented\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdba3d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43945294",
   "metadata": {},
   "source": [
    "### Vocabulary Comparison (Table 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e98a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabularies of the models\n",
    "\n",
    "unigram_default_vocab = [unigram_default.id_to_piece(id) for id in range(unigram_default.get_piece_size())]\n",
    "unigram_prime_vocab = [unigram_prime.id_to_piece(id) for id in range(unigram_prime.get_piece_size())]\n",
    "bpe_default_vocab = [bpe_default.id_to_piece(id) for id in range(bpe_default.get_piece_size())]\n",
    "bpe_prime_vocab = [bpe_prime.id_to_piece(id) for id in range(bpe_prime.get_piece_size())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf4b05bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate items from default Unigram and BPE vocabularies\n",
    "unigram_default_vocab_no_duplicates = set([x.lstrip('▁') if x != \"▁\" else x for x in unigram_default_vocab])\n",
    "bpe_default_vocab_no_duplicates = set([x.lstrip('▁') if x != \"▁\" else x for x in bpe_default_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6408afb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.650000000000002"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of duplicate items for default BPE\n",
    "100*(1 - len(bpe_default_vocab_no_duplicates) / len(bpe_default_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c38ff64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.081249999999997"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentage of duplicate items for default Unigram\n",
    "100*(1 - len(unigram_default_vocab_no_duplicates) / len(unigram_default_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a5f05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of prefixes taken from Wikipedia\n",
    "prefixes = [\"a-\",\n",
    "\"after-\",\n",
    "\"anti-\",\n",
    "\"back-\",\n",
    "\"be-\",\n",
    "\"by-\",\n",
    "\"co-\",\n",
    "\"de-\",\n",
    "\"dis-\",\n",
    "\"dis-\",\n",
    "\"down-\",\n",
    "\"en-,\",\n",
    "\"em\",\n",
    "\"ex-\",\n",
    "\"fore-\",\n",
    "\"hind-\",\n",
    "\"mid-\",\n",
    "\"midi-\",\n",
    "\"mini-\",\n",
    "\"mis-\",\n",
    "\"off-\",\n",
    "\"on-\",\n",
    "\"out-\",\n",
    "\"over-\",\n",
    "\"post-\",\n",
    "\"pre-\",\n",
    "\"pro-\",\n",
    "\"re-\",\n",
    "\"self-\",\n",
    "\"step-\",\n",
    "\"twi-\",\n",
    "\"un-\",\n",
    "\"un-\",\n",
    "\"under-\",\n",
    "\"up-\",\n",
    "\"with-\",\n",
    "\"a-\",\n",
    "\"Afro-\",\n",
    "\"ambi-\",\n",
    "\"amphi-\",\n",
    "\"an-\",\n",
    "\"a\",\n",
    "\"ana-,\",\n",
    "\"Anglo-\",\n",
    "\"ante-\",\n",
    "\"anti-\",\n",
    "\"apo-,\",\n",
    "\"ap\",\n",
    "\"arch-\",\n",
    "\"astro-\",\n",
    "\"auto-\",\n",
    "\"bi-\",\n",
    "\"bio-\",\n",
    "\"circum-\",\n",
    "\"cis-\",\n",
    "\"con-,\",\n",
    "\"co\",\n",
    "\"com\",\n",
    "\"col\", \n",
    "\"cor\",\n",
    "\"contra-,\",\n",
    "\"contro\",\n",
    "\"counter-\",\n",
    "\"cryo-\",\n",
    "\"crypto-\",\n",
    "\"de-\",\n",
    "\"demi-\",\n",
    "\"demo-\",\n",
    "\"deuter-\",\n",
    "\"di-\",\n",
    "\"dia-\",\n",
    "\"dis-\",\n",
    "\"di\",\n",
    "\"dif\",\n",
    "\"du-\",\n",
    "\"duo-\",\n",
    "\"eco-\",\n",
    "\"electro-\",\n",
    "\"en-,\",\n",
    "\"el\",\n",
    "\"em\",\n",
    "\"epi-,\",\n",
    "\"ep\",\n",
    "\"Euro-\",\n",
    "\"ex-\",\n",
    "\"extra-\",\n",
    "\"Franco-\",\n",
    "\"geo-\",\n",
    "\"gyro-\",\n",
    "\"hetero-\",\n",
    "\"hemi-\",\n",
    "\"homo-\",\n",
    "\"hydro-\",\n",
    "\"hyper-\",\n",
    "\"hypo-\",\n",
    "\"ideo-\",\n",
    "\"idio-\",\n",
    "\"in-\",\n",
    "\"Indo-\",\n",
    "\"in-,\",\n",
    "\"il\",\n",
    "\"im\",\n",
    "\"ir\",\n",
    "\"infra-\",\n",
    "\"inter-\",\n",
    "\"intra-\",\n",
    "\"iso-\",\n",
    "\"macro-\",\n",
    "\"mal-\",\n",
    "\"maxi-\",\n",
    "\"mega-,\",\n",
    "\"megalo\",\n",
    "\"meta-\",\n",
    "\"micro-\",\n",
    "\"mono-,\",\n",
    "\"mon\",\n",
    "\"multi-,\",\n",
    "\"mult\",\n",
    "\"neo-\",\n",
    "\"non-\",\n",
    "\"ob-\",\n",
    "\"omni-\",\n",
    "\"ortho-\",\n",
    "\"paleo-\",\n",
    "\"pan-\",\n",
    "\"para-\",\n",
    "\"ped-\",\n",
    "\"pen-\",\n",
    "\"per-\",\n",
    "\"peri-\",\n",
    "\"photo-\",\n",
    "\"pleo-\",\n",
    "\"pod-\",\n",
    "\"poly-\",\n",
    "\"post-\",\n",
    "\"pre-\",\n",
    "\"preter-\",\n",
    "\"pro-\",\n",
    "\"pro-\",\n",
    "\"pros-\",\n",
    "\"proto-\",\n",
    "\"pseudo-\",\n",
    "\"pyro-\",\n",
    "\"quadri-\",\n",
    "\"quasi-\",\n",
    "\"retro-\",\n",
    "\"semi-\",\n",
    "\"socio-\",\n",
    "\"sub-,\",\n",
    "\"sup\",\n",
    "\"super-\",\n",
    "\"supra-\",\n",
    "\"sur-\",\n",
    "\"syn-,\",\n",
    "\"sy\",\n",
    "\"syl\",\n",
    "\"sym\",\n",
    "\"tele-\",\n",
    "\"trans-\",\n",
    "\"tri-\",\n",
    "\"ultra-\",\n",
    "\"uni-\",\n",
    "\"vice-\",]\n",
    "prefixes = [a.rstrip(',').rstrip('-') for a in prefixes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00626039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of suffixes taken from Wikipedia\n",
    "suffixes = [\"-a\",\n",
    "\"-ability\",\n",
    "\"-able\",\n",
    "\"-ably\",\n",
    "\"-ac\",\n",
    "\"-acean\",\n",
    "\"-aceous\",\n",
    "\"-ad\",\n",
    "\"-ade\",\n",
    "\"-aemia\",\n",
    "\"-age\",\n",
    "\"-agog\",\n",
    "\"-aholic\",\n",
    "\"-al\",\n",
    "\"-algia\",\n",
    "\"-all\",\n",
    "\"-amine\",\n",
    "\"-an\",\n",
    "\"-ana\",\n",
    "\"-ance\",\n",
    "\"-ancy\",\n",
    "\"-androus\",\n",
    "\"-andry\",\n",
    "\"-ane\",\n",
    "\"-ant\",\n",
    "\"-ar\",\n",
    "\"-arch\",\n",
    "\"-archy\",\n",
    "\"-ard\",\n",
    "\"-arian\",\n",
    "\"-arium\",\n",
    "\"-art\",\n",
    "\"-ary\",\n",
    "\"-ase\",\n",
    "\"-ate\",\n",
    "\"-athon\",\n",
    "\"-ation\",\n",
    "\"-ative\",\n",
    "\"-ator\",\n",
    "\"-atory\",\n",
    "\"B\",\n",
    "\"-biont\",\n",
    "\"-biosis\",\n",
    "\"-blast\",\n",
    "\"-bot\",\n",
    "\"C\",\n",
    "\"-cade\",\n",
    "\"-caine\",\n",
    "\"-carp\",\n",
    "\"-carpic\",\n",
    "\"-carpous\",\n",
    "\"-cele\",\n",
    "\"-cene\",\n",
    "\"-centric\",\n",
    "\"-cephalic\",\n",
    "\"-cephalous\",\n",
    "\"-cephaly\",\n",
    "\"-chore\",\n",
    "\"-chory\",\n",
    "\"-chrome\",\n",
    "\"-cide\",\n",
    "\"-clast\",\n",
    "\"-clinal\",\n",
    "\"-cline\",\n",
    "\"-clinic\",\n",
    "\"-coccus\",\n",
    "\"-coel\",\n",
    "\"-coele\",\n",
    "\"-colous\",\n",
    "\"-cracy\",\n",
    "\"-crat\",\n",
    "\"-cratic,\",\n",
    "\"-cy\",\n",
    "\"-cyte\",\n",
    "\"D\",\n",
    "\"-dale\",\n",
    "\"-derm\",\n",
    "\"-derma\",\n",
    "\"-dermatous\",\n",
    "\"-dom\",\n",
    "\"-drome\",\n",
    "\"-dromous\",\n",
    "\"E\",\n",
    "\"-ean\",\n",
    "\"-eaux\",\n",
    "\"-ectomy\",\n",
    "\"-ed\",\n",
    "\"-ee\",\n",
    "\"-eer\",\n",
    "\"-ein\",\n",
    "\"-eme\",\n",
    "\"-emia\",\n",
    "\"-en\",\n",
    "\"-ence\",\n",
    "\"-enchyma\",\n",
    "\"-ency\",\n",
    "\"-ene\",\n",
    "\"-ent\",\n",
    "\"-eous\",\n",
    "\"-er\",\n",
    "\"-ergic\",\n",
    "\"-ergy\",\n",
    "\"-es\",\n",
    "\"-escence\",\n",
    "\"-escent\",\n",
    "\"-ese\",\n",
    "\"-esque\",\n",
    "\"-ess\",\n",
    "\"-est\",\n",
    "\"-et\",\n",
    "\"-eth\",\n",
    "\"-etic\",\n",
    "\"-ette\",\n",
    "\"-ey\",\n",
    "\"F\",\n",
    "\"-facient\",\n",
    "\"-faction\",\n",
    "\"-fer\",\n",
    "\"-ferous\",\n",
    "\"-fic\",\n",
    "\"-fication\",\n",
    "\"-fid\",\n",
    "\"-florous\",\n",
    "\"-fold\",\n",
    "\"-foliate\",\n",
    "\"-foliolate\",\n",
    "\"-form\",\n",
    "\"-fuge\",\n",
    "\"-ful\",\n",
    "\"-fy\",\n",
    "\"G\",\n",
    "\"-gamous\",\n",
    "\"-gamy\",\n",
    "\"-gate\",\n",
    "\"-gen,\",\n",
    "\"-genesis\",\n",
    "\"-genetic\",\n",
    "\"-genic\",\n",
    "\"-genous\",\n",
    "\"-geny\",\n",
    "\"-gnathous\",\n",
    "\"-gon\",\n",
    "\"-gony\",\n",
    "\"-gram\",\n",
    "\"-graph\",\n",
    "\"-grapher\",\n",
    "\"-graphy\",\n",
    "\"-gyne\",\n",
    "\"-gynous\",\n",
    "\"-gyny\",\n",
    "\"H\",\n",
    "\"-hood\",\n",
    "\"I\",\n",
    "\"-ia\",\n",
    "\"-ial\",\n",
    "\"-ian\",\n",
    "\"-iana\",\n",
    "\"-iasis\",\n",
    "\"-iatric\",\n",
    "\"-iatrics\",\n",
    "\"-iatry\",\n",
    "\"-ibility\",\n",
    "\"-ible\",\n",
    "\"-ic\",\n",
    "\"-icide\",\n",
    "\"-ician\",\n",
    "\"-ick\",\n",
    "\"-ics\",\n",
    "\"-id\",\n",
    "\"-ide\",\n",
    "\"-ie\",\n",
    "\"-ify\",\n",
    "\"-ile\",\n",
    "\"-in\",\n",
    "\"-ine\",\n",
    "\"-ing\",\n",
    "\"-ion\",\n",
    "\"-ious\",\n",
    "\"-isation\",\n",
    "\"-ise\",\n",
    "\"-ish\",\n",
    "\"-ism\",\n",
    "\"-ist\",\n",
    "\"-istic\",\n",
    "\"-istical\",\n",
    "\"-istically\",\n",
    "\"-ite\",\n",
    "\"-itious\",\n",
    "\"-itis\",\n",
    "\"-ity\",\n",
    "\"-ium\",\n",
    "\"-ive\",\n",
    "\"-ix\",\n",
    "\"-ization\",\n",
    "\"-ize\",\n",
    "\"-i-\",\n",
    "\"J\",\n",
    "\"K\",\n",
    "\"-kin\",\n",
    "\"-kinesis\",\n",
    "\"-kins\",\n",
    "\"L\",\n",
    "\"-land\",\n",
    "\"-latry\",\n",
    "\"-le\",\n",
    "\"-lepry\",\n",
    "\"-less\",\n",
    "\"-let\",\n",
    "\"-like\",\n",
    "\"-ling\",\n",
    "\"-lite\",\n",
    "\"-lith\",\n",
    "\"-lithic\",\n",
    "\"-log\",\n",
    "\"-logic\",\n",
    "\"-logical\",\n",
    "\"-logist\",\n",
    "\"-logy\",\n",
    "\"-ly\",\n",
    "\"-lyse\",\n",
    "\"-lysis\",\n",
    "\"-lyte\",\n",
    "\"-lytic\",\n",
    "\"-lyze\",\n",
    "\"M\",\n",
    "\"-mancy\",\n",
    "\"-mania\",\n",
    "\"-meister\",\n",
    "\"-ment\",\n",
    "\"-mer\",\n",
    "\"-mere\",\n",
    "\"-merous\",\n",
    "\"-meter\",\n",
    "\"-metric\",\n",
    "\"-metrics\",\n",
    "\"-metry\",\n",
    "\"-mire\",\n",
    "\"-mo\",\n",
    "\"-morph\",\n",
    "\"-morphic\",\n",
    "\"-morphism\",\n",
    "\"-morphous\",\n",
    "\"-most\",\n",
    "\"-mycete\",\n",
    "\"-mycin\",\n",
    "\"N\",\n",
    "\"-n't\",\n",
    "\"-nasty\",\n",
    "\"-ness\",\n",
    "\"-nik\",\n",
    "\"-nomy\",\n",
    "\"-nomics\",\n",
    "\"O\",\n",
    "\"-o\",\n",
    "\"-ode\",\n",
    "\"-odon\",\n",
    "\"-odont\",\n",
    "\"-odontia\",\n",
    "\"-oholic\",\n",
    "\"-oic\",\n",
    "\"-oid\",\n",
    "\"-ol\",\n",
    "\"-ole\",\n",
    "\"-oma\",\n",
    "\"-ome\",\n",
    "\"-omics\",\n",
    "\"-on\",\n",
    "\"-one\",\n",
    "\"-ont\",\n",
    "\"-onym\",\n",
    "\"-onymy\",\n",
    "\"-opia\",\n",
    "\"-opsis\",\n",
    "\"-opsy\",\n",
    "\"-or\",\n",
    "\"-orama\",\n",
    "\"-ory\",\n",
    "\"-ose\",\n",
    "\"-osis\",\n",
    "\"-otic\",\n",
    "\"-otomy\",\n",
    "\"-ous\",\n",
    "\"-o-\",\n",
    "\"P\",\n",
    "\"-para\",\n",
    "\"-parous\",\n",
    "\"-path\",\n",
    "\"-pathy\",\n",
    "\"-ped\",\n",
    "\"-pede\",\n",
    "\"-penia\",\n",
    "\"-petal\",\n",
    "\"-phage\",\n",
    "\"-phagia\",\n",
    "\"-phagous\",\n",
    "\"-phagy\",\n",
    "\"-phane\",\n",
    "\"-phasia\",\n",
    "\"-phil\",\n",
    "\"-phile\",\n",
    "\"-philia\",\n",
    "\"-philiac\",\n",
    "\"-philic\",\n",
    "\"-philous\",\n",
    "\"-phobe\",\n",
    "\"-phobia\",\n",
    "\"-phobic\",\n",
    "\"-phone\",\n",
    "\"-phony\",\n",
    "\"-phore\",\n",
    "\"-phoresis\",\n",
    "\"-phorous\",\n",
    "\"-phrenia\",\n",
    "\"-phyll\",\n",
    "\"-phyllous\",\n",
    "\"-plasia\",\n",
    "\"-plasm\",\n",
    "\"-plast\",\n",
    "\"-plastic\",\n",
    "\"-plasty\",\n",
    "\"-plegia\",\n",
    "\"-plex\",\n",
    "\"-ploid\",\n",
    "\"-pod\",\n",
    "\"-pode\",\n",
    "\"-podous\",\n",
    "\"-poieses\",\n",
    "\"-poietic\",\n",
    "\"-pter\",\n",
    "\"-punk\",\n",
    "\"Q\",\n",
    "\"R\",\n",
    "\"-rrhagia\",\n",
    "\"-rrhea\",\n",
    "\"-ric\",\n",
    "\"-ry\",\n",
    "\"S\",\n",
    "\"-'s\",\n",
    "\"-s\",\n",
    "\"-scape\",\n",
    "\"-scope\",\n",
    "\"-scopy\",\n",
    "\"-script\",\n",
    "\"-sect\",\n",
    "\"-sepalous\",\n",
    "\"-ship\",\n",
    "\"-some\",\n",
    "\"-speak\",\n",
    "\"-sperm\",\n",
    "\"-sphere\",\n",
    "\"-sporous\",\n",
    "\"-st\",\n",
    "\"-stasis\",\n",
    "\"-stat\",\n",
    "\"-ster\",\n",
    "\"-stome\",\n",
    "\"-stomy\",\n",
    "\"T\",\n",
    "\"-taxis\",\n",
    "\"-taxy\",\n",
    "\"-tend\",\n",
    "\"-th\",\n",
    "\"-therm\",\n",
    "\"-thermal,\",\n",
    "\"-thermy\",\n",
    "\"-thon\",\n",
    "\"-thymia\",\n",
    "\"-tion\",\n",
    "\"-tome\",\n",
    "\"-tomy\",\n",
    "\"-tonia\",\n",
    "\"-trichous\",\n",
    "\"-trix\",\n",
    "\"-tron\",\n",
    "\"-trophic\",\n",
    "\"-trophy\",\n",
    "\"-tropic\",\n",
    "\"-tropism\",\n",
    "\"-tropous\",\n",
    "\"-tropy\",\n",
    "\"-tude\",\n",
    "\"-ture\",\n",
    "\"-ty\",\n",
    "\"U\",\n",
    "\"-ular\",\n",
    "\"-ule\",\n",
    "\"-ure\",\n",
    "\"-urgy\",\n",
    "\"-uria\",\n",
    "\"-uronic\",\n",
    "\"-urous\",\n",
    "\"V\",\n",
    "\"-valent\",\n",
    "\"-virile\",\n",
    "\"-vorous\",\n",
    "\"W\",\n",
    "\"-ward\",\n",
    "\"-wards\",\n",
    "\"-ware\",\n",
    "\"-ways\",\n",
    "\"-wear\",\n",
    "\"-wide\",\n",
    "\"-wise\",\n",
    "\"-worthy\",\n",
    "\"X\",\n",
    "\"-xor\",\n",
    "\"Y\",\n",
    "\"-y\",\n",
    "\"-yl\",\n",
    "\"-yne\",\n",
    "\"Z\",\n",
    "\"-zilla\",\n",
    "\"-zoic\",\n",
    "\"-zoon\",\n",
    "\"-zygous\",\n",
    "\"-zyme\",]\n",
    "suffixes = [a.lstrip('-') for a in suffixes if a[0] == '-']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
